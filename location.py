# -*- coding: utf-8 -*-
"""location.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vtPZvwvyVMKzkgwKtSPJBvz7vxCgxPL5
"""



pip install folium geopandas pandas

attr="&copy; <a href='https://www.openstreetmap.org/copyright'>OpenStreetMap contributors</a>"

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

!pip install matplotlib seaborn pandas

import folium
from folium.plugins import HeatMap
import pandas as pd
from shapely.geometry import Point

# Function to plot traffic density heatmap
def plot_interactive_heatmap(gdf, density_column='noisy_density', map_center=[60.472, 8.468], zoom_start=6):
    """
    Create an interactive heatmap for traffic density using Folium with proper tile attribution.
    """
    # Create a base map centered on Norway
    m = folium.Map(
        location=map_center,
        zoom_start=zoom_start,
        tiles="OpenStreetMap",
        attr='&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
    )

    # Normalize traffic density for better heatmap representation
    gdf['normalized_density'] = gdf[density_column] / gdf[density_column].max()

    # Prepare heatmap data
    heatmap_data = [
        [row['latitude'], row['longitude'], row['normalized_density']]
        for _, row in gdf.iterrows()
    ]

    # Add heatmap layer
    # Convert gradient keys to strings
    gradient = {str(k): v for k, v in {0: 'green', 0.5: 'yellow', 1: 'red'}.items()}
    HeatMap(
        heatmap_data,
        radius=15,  # Radius of each point
        blur=10,  # Blurring factor
        max_zoom=12,  # Maximum zoom for the heatmap
        gradient=gradient  # Gradient for density with string keys
    ).add_to(m)

    # Add markers for each region
    for _, row in gdf.iterrows():
        popup_text = f"""
        <strong>Region:</strong> {row['region']}<br>
        <strong>Density:</strong> {row[density_column]:.2f}
        """
        folium.CircleMarker(
            location=[row['latitude'], row['longitude']],
            radius=5,
            color='blue' if row[density_column] < 100 else 'red',
            fill=True,
            fill_opacity=0.7,
            popup=folium.Popup(popup_text, max_width=250)
        ).add_to(m)

    return m

# Example usage:
# Ensure traffic_gdf is prepared with required columns: 'latitude', 'longitude', 'noisy_density', 'region'
norway_traffic_map = plot_interactive_heatmap(traffic_gdf, density_column='noisy_density')

# Save or display the map
norway_traffic_map.save("norway_traffic_heatmap2.html")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Define 50 Regions with Norwegian Cities
norwegian_cities = [
    "Oslo", "Bergen", "Trondheim", "Stavanger", "Kristiansand", "Tromsø", "Sandnes", "Drammen", "Fredrikstad", "Porsgrunn",
    "Ålesund", "Haugesund", "Tønsberg", "Moss", "Bodø", "Arendal", "Hamar", "Halden", "Narvik", "Gjøvik",
    "Lillehammer", "Harstad", "Molde", "Kongsberg", "Mo i Rana", "Steinkjer", "Alta", "Sandefjord", "Sarpsborg", "Svolvær",
    "Vadsø", "Vardø", "Røros", "Florø", "Førde", "Voss", "Odda", "Levanger", "Flekkefjord", "Grimstad",
    "Notodden", "Namsos", "Hammerfest", "Kautokeino", "Egersund", "Mandals", "Holmestrand", "Stjørdal", "Kongsvinger", "Rakkestad"
]

# Step 2: Simulate Traffic Data with Peak Hour Congestion
def simulate_traffic_data_peak_hours(regions, time_intervals, avg_density=100, density_scale=20):
    data = []
    for region in regions:
        for t in time_intervals:
            # Simulate peak-hour congestion for cities
            if region in norwegian_cities[:20] and (5.5 <= t < 8 or 16 <= t < 18):
                density = np.random.poisson(lam=avg_density + 100 + np.random.normal(0, density_scale))
            else:
                density = np.random.poisson(lam=avg_density + np.random.normal(0, density_scale))
            data.append({'region': region, 'timestamp': t, 'density': density})
    return pd.DataFrame(data)

# Generate 50 regions and 24 hourly timestamps
regions = norwegian_cities
time_intervals = np.arange(0, 24, 1)  # Hourly intervals
traffic_data = simulate_traffic_data_peak_hours(regions, time_intervals)
# Save the generated traffic data to a CSV file
traffic_data.to_csv('traffic_data.csv', index=False)

# Step 3: Query Function
def query_data(df, region, timestamp):
    return df[(df['region'] == region) & (df['timestamp'] == timestamp)]

# Step 4: Global and Local Shuffling
def global_shuffling(df):
    return df.sample(frac=1).reset_index(drop=True)

def local_shuffling(df, group_col):
    shuffled_groups = []
    for group, group_data in df.groupby(group_col):
        shuffled_groups.append(group_data.sample(frac=1).reset_index(drop=True))
    return pd.concat(shuffled_groups).reset_index(drop=True)

# Step 5: Inject Laplace Noise
def inject_laplace_noise(df, column, epsilon, sensitivity=1.0):
    b = sensitivity / epsilon
    noise = np.random.laplace(0, b, size=len(df))
    df[f'noisy_{column}'] = df[column] + noise
    return df

optimal_epsilon = 1.0  # Privacy budget
optimal_noisy_data = inject_laplace_noise(traffic_data.copy(), column='density', epsilon=optimal_epsilon)

# Step 6: Experiment with Privacy Budgets
def test_privacy_budgets(df, column, budgets):
    results = []
    for epsilon in budgets:
        noisy_df = inject_laplace_noise(df.copy(), column, epsilon)
        mse = mean_squared_error(df[column], noisy_df[f'noisy_{column}'])
        mae = mean_absolute_error(df[column], noisy_df[f'noisy_{column}'])
        results.append({'epsilon': epsilon, 'MSE': mse, 'MAE': mae})
    return pd.DataFrame(results)

privacy_budgets = np.linspace(0.1, 2.0, 10)  # Test budgets from 0.1 to 2.0
privacy_results = test_privacy_budgets(traffic_data, column='density', budgets=privacy_budgets)

# Step 7: Identify Optimal Epsilon
def find_optimal_epsilon(results):
    optimal_epsilon = results.loc[results['MSE'].idxmin(), 'epsilon']
    print(f"Optimal Privacy Budget (Epsilon): {optimal_epsilon}")
    return optimal_epsilon

optimal_epsilon = find_optimal_epsilon(privacy_results)

# Step 8: Plot Privacy-Utility Trade-Off
def plot_privacy_tradeoff(results):
    plt.figure(figsize=(10, 6))
    plt.plot(results['epsilon'], results['MSE'], label='Mean Squared Error (MSE)', marker='o')
    plt.plot(results['epsilon'], results['MAE'], label='Mean Absolute Error (MAE)', marker='x')
    plt.axvline(x=optimal_epsilon, color='r', linestyle='--', label='Optimal Epsilon')
    plt.title('Privacy-Utility Trade-off')
    plt.xlabel('Privacy Budget (Epsilon)')
    plt.ylabel('Error')
    plt.legend()
    plt.grid()
    plt.show()

plot_privacy_tradeoff(privacy_results)

# Step 9: Compare Original and Noisy Densities
def plot_density_comparison(df, original_column, noisy_column):
    plt.figure(figsize=(12, 6))
    sns.kdeplot(df[original_column], label='Original Density', shade=True, color='blue')
    sns.kdeplot(df[noisy_column], label='Noisy Density', shade=True, color='orange')
    plt.title('Density Distribution: Original vs Noisy')
    plt.xlabel('Traffic Density')
    plt.ylabel('Density')
    plt.legend()
    plt.show()

plot_density_comparison(optimal_noisy_data, original_column='density', noisy_column='noisy_density')

# Step 10: Fairness Analysis
def analyze_group_representation(df, group_col, original_column, noisy_column):
    original_means = df.groupby(group_col)[original_column].mean()
    noisy_means = df.groupby(group_col)[noisy_column].mean()
    fairness_df = pd.DataFrame({'Original Mean': original_means, 'Noisy Mean': noisy_means})
    fairness_df['Difference'] = fairness_df['Original Mean'] - fairness_df['Noisy Mean']
    return fairness_df

fairness_analysis = analyze_group_representation(optimal_noisy_data, group_col='region', original_column='density', noisy_column='noisy_density')

def plot_fairness_comparison(fairness_df):
    fairness_df[['Original Mean', 'Noisy Mean']].plot(kind='bar', figsize=(12, 6))
    plt.title('Fairness Comparison: Original vs Noisy Means')
    plt.xlabel('Region')
    plt.ylabel('Traffic Density')
    plt.xticks(rotation=90)
    plt.legend()
    plt.show()

plot_fairness_comparison(fairness_analysis)

# Step 11: Predict Region-Wise Traffic for Next 24 Hours
def predict_region_traffic(df, regions, timestamps, noisy_column=None):
    predictions = []
    for region in regions:
        for t in timestamps:
            if noisy_column:
                density = df[(df['region'] == region) & (df['timestamp'] == t)][noisy_column].values[0]
            else:
                density = df[(df['region'] == region) & (df['timestamp'] == t)]['density'].values[0]
            predictions.append({'region': region, 'timestamp': t, 'predicted_density': density})
    return pd.DataFrame(predictions)

selected_regions = np.random.choice(regions, size=5, replace=False)
timestamps = np.arange(0, 24)  # Next 24 hours
original_predictions = predict_region_traffic(traffic_data, selected_regions, timestamps)
noisy_predictions = predict_region_traffic(optimal_noisy_data, selected_regions, timestamps, noisy_column='noisy_density')

# Step 12: Combine and Plot Region Predictions
def prepare_combined_predictions(original_df, noisy_df):
    original_df['type'] = 'Original'
    noisy_df['type'] = 'Noisy'
    return pd.concat([original_df, noisy_df])

combined_predictions = prepare_combined_predictions(original_predictions, noisy_predictions)

def plot_region_predictions_combined(combined_df):
    plt.figure(figsize=(14, 8))
    sns.lineplot(data=combined_df, x='timestamp', y='predicted_density', hue='region', style='type', marker='o')
    plt.title('Region-Wise Traffic Predictions for Next 24 Hours')
    plt.xlabel('Timestamp (hour)')
    plt.ylabel('Traffic Density')
    plt.legend(title='Region and Type', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid()
    plt.show()

plot_region_predictions_combined(combined_predictions)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def plot_traffic_density_heatmap(df, timestamp, noisy_column='noisy_density', density_threshold=150):
    # Filter the DataFrame for the given timestamp
    traffic_at_t = df[df['timestamp'] == timestamp].copy()

    # Determine traffic status based on the density threshold
    traffic_at_t['traffic_status'] = traffic_at_t[noisy_column].apply(
        lambda x: 'Congested' if x > density_threshold else 'Free'
    )

    # Map traffic status to numeric values: 1 for Congested, 0 for Free
    traffic_at_t['heat_value'] = traffic_at_t['traffic_status'].map({'Congested': 1, 'Free': 0})

    # Use pivot_table to create the heatmap matrix
    # The aggfunc='first' is used here to manage duplicate entries for the same region and timestamp, if any
    heatmap_data = traffic_at_t.pivot_table(index='region', values='heat_value', columns='timestamp', aggfunc='first').fillna(0)

    # Create the heatmap
    plt.figure(figsize=(12, 8))
    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt='.0f',
        cmap='RdYlGn_r',  # Reversed color map to align colors intuitively
        linewidths=0.5,
        cbar_kws={'label': 'Traffic Status (0=Free, 1=Congested)'}
    )
    plt.title(f'Traffic Density Heatmap at Time {timestamp}:00', fontsize=16)
    plt.xlabel('Hour of the Day')
    plt.ylabel('Region', fontsize=12)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

# Example usage assuming 'optimal_noisy_data' is properly defined
# Make sure to replace 'optimal_noisy_data' with your actual DataFrame name if different
plot_traffic_density_heatmap(optimal_noisy_data, timestamp=7, noisy_column='noisy_density')
plot_traffic_density_heatmap(optimal_noisy_data, timestamp=17, noisy_column='noisy_density')



import folium
from folium.plugins import HeatMap
import folium

# Define the map with restricted bounds
m = folium.Map(
    location=[65, 15],  # Center of Norway
    zoom_start=6,
    max_bounds=True,
    min_lat=58,
    max_lat=71,
    min_lon=5,
    max_lon=31
)


# Add predefined latitude and longitude for Norwegian cities
import numpy as np

def add_geolocation(df):
    """
    Add latitude and longitude to the DataFrame for predefined cities.
    """
    predefined_coordinates = {
        "Oslo": (59.9139, 10.7522),
        "Bergen": (60.3928, 5.3239),
        "Trondheim": (63.4305, 10.3951),
        "Stavanger": (58.9690, 5.7331),
        "Kristiansand": (58.1467, 7.9956),
        "Tromsø": (69.6496, 18.9560),
        "Sandnes": (58.8524, 5.7352),
        "Drammen": (59.7439, 10.2045),
        "Fredrikstad": (59.2181, 10.9298),
        "Porsgrunn": (59.1390, 9.6561),
        "Ålesund": (62.4722, 6.1549),
        "Haugesund": (59.4138, 5.2680),
        "Tønsberg": (59.2675, 10.4076),
        "Moss": (59.4340, 10.6577),
        "Bodø": (67.2804, 14.4049),
        "Arendal": (58.4615, 8.7720),
        "Hamar": (60.7945, 11.0670),
        "Halden": (59.1200, 11.3875),
        "Narvik": (68.4385, 17.4272),
        "Gjøvik": (60.7959, 10.6917)
    }

    df['latitude'] = df['region'].apply(lambda x: predefined_coordinates[x][0] if x in predefined_coordinates else np.nan)
    df['longitude'] = df['region'].apply(lambda x: predefined_coordinates[x][1] if x in predefined_coordinates else np.nan)
    return df

    df = df.dropna(subset=['latitude', 'longitude'])

    import folium

# Define the map with restricted bounds
m = folium.Map(
    location=[65, 15],  # Center of Norway
    zoom_start=6,
    max_bounds=True
)

# Set the maximum bounds
m.fit_bounds([[58, 5], [71, 31]])


# Add geolocation to the noisy data
optimal_noisy_data = add_geolocation(optimal_noisy_data)

def sanitize_dataframe(df):
    """
    Ensure all column names and keys in the DataFrame are compatible with Folium.
    """
    df.columns = df.columns.map(str)  # Convert column names to strings
    df['region'] = df['region'].astype(str)  # Ensure region names are strings
    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')  # Ensure latitude is numeric
    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')  # Ensure longitude is numeric
    df['noisy_density'] = pd.to_numeric(df['noisy_density'], errors='coerce')  # Ensure density is numeric
    df = df.dropna()  # Drop rows with NaN values
    return df

# Apply sanitization to the noisy data
optimal_noisy_data = sanitize_dataframe(optimal_noisy_data)

# Step 4: Sanitize DataFrame for Folium
def sanitize_dataframe(df):
    df.columns = df.columns.map(str)
    df['region'] = df['region'].astype(str)
    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
    df['noisy_density'] = pd.to_numeric(df['noisy_density'], errors='coerce')
    df = df.dropna()
    return df
optimal_noisy_data = sanitize_dataframe(optimal_noisy_data)

def filter_norway_boundaries(df):
    # Norway's mainland geographical bounds
    lat_min, lat_max = 58, 71
    lon_min, lon_max = 5, 31
    return df[(df['latitude'] >= lat_min) & (df['latitude'] <= lat_max) &
              (df['longitude'] >= lon_min) & (df['longitude'] <= lon_max)]

optimal_noisy_data = filter_norway_boundaries(optimal_noisy_data)




import folium
from folium.plugins import HeatMap
import pandas as pd
from shapely.geometry import Point



# Step 5: Interactive Heatmap Function
def plot_interactive_heatmap(df, density_column='noisy_density', map_center=[60.472, 8.468], zoom_start=6):
    df['normalized_density'] = df[density_column] / df[density_column].max()
    m = folium.Map(location=map_center, zoom_start=zoom_start, tiles="OpenStreetMap")
    heatmap_data = [
        [row['latitude'], row['longitude'], row['normalized_density']]
        for _, row in df.iterrows()
    ]
    # Convert gradient keys to strings
    gradient = {str(k): v for k, v in {0: 'green', 0.5: 'yellow', 1: 'red'}.items()}
    HeatMap(
        heatmap_data,
        radius=15,
        blur=10,
        max_zoom=12,
        gradient=gradient # Use the gradient with string keys
    ).add_to(m)
    for _, row in df.iterrows():
        popup_text = f"""
        <strong>Region:</strong> {row['region']}<br>
        <strong>Density:</strong> {row[density_column]:.2f}
        """
        folium.CircleMarker(
            location=[row['latitude'], row['longitude']],
            radius=5,
            color='blue' if row[density_column] < 100 else 'red',
            fill=True,
            fill_opacity=0.7,
            popup=folium.Popup(popup_text, max_width=250)
        ).add_to(m)
    return m

# ... (The rest of your code) ...

# Step 6: Create and Save the Interactive Map
norway_traffic_map = plot_interactive_heatmap(optimal_noisy_data, density_column='noisy_density')
norway_traffic_map.save("norway_traffic_heatmap3.html")

import numpy as np
import pandas as pd
import folium
from folium.plugins import HeatMapWithTime

# Sample data preparation
# Replace this with your actual data loading mechanism
# Ensure your DataFrame has 'region', 'timestamp', 'latitude', 'longitude', and 'density' columns
data = {
    'region': ['Oslo', 'Bergen', 'Trondheim', 'Oslo', 'Bergen', 'Trondheim'],
    'timestamp': [0, 0, 0, 1, 1, 1],  # Example timestamps representing hours
    'latitude': [59.9139, 60.3928, 63.4305, 59.9139, 60.3928, 63.4305],
    'longitude': [10.7522, 5.3239, 10.3951, 10.7522, 5.3239, 10.3951],
    'density': [120, 80, 95, 130, 85, 100]
}
df = pd.DataFrame(data)

# Normalize the density values for heatmap intensity
df['normalized_density'] = df['density'] / df['density'].max()

# Prepare data for HeatMapWithTime
time_index = sorted(df['timestamp'].unique())
heat_data = []

for t in time_index:
    data_at_t = df[df['timestamp'] == t]
    heat_data.append(data_at_t[['latitude', 'longitude', 'normalized_density']].values.tolist())

# Create a Folium map centered on Norway
m = folium.Map(location=[65, 15], zoom_start=6)

# Add the HeatMapWithTime
HeatMapWithTime(
    data=heat_data,
    index=[f'Hour {t}:00' for t in time_index],  # Labeling each hour
    radius=15,
    auto_play=False,
    max_opacity=0.8
).add_to(m)

# Save the map to an HTML file
m.save('norway_traffic_heatmap_with_time.html')

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Ensure fairness analysis data is prepared
fairness_analysis = analyze_group_representation(
    optimal_noisy_data,
    group_col='region',
    original_column='density',
    noisy_column='noisy_density'
)

# Fairness Difference Plot
def plot_fairness_difference(fairness_df):
    """
    Plot the absolute differences between original and noisy means.
    """
    fairness_df['Absolute Difference'] = fairness_df['Difference'].abs()
    fairness_df.sort_values(by='Absolute Difference', ascending=False, inplace=True)

    plt.figure(figsize=(14, 6))
    sns.barplot(
        data=fairness_df.reset_index(),
        x='region',
        y='Absolute Difference',
        palette='viridis'
    )
    plt.title('Absolute Difference Between Original and Noisy Means (Fairness Analysis)', fontsize=16)
    plt.xlabel('Region', fontsize=12)
    plt.ylabel('Absolute Difference', fontsize=12)
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

# Fairness Boxplot
def plot_fairness_boxplot(df, original_column, noisy_column, group_col='region'):
    """
    Compare the distributions of original and noisy traffic densities across all regions.
    """
    melted_df = pd.melt(
        df[[group_col, original_column, noisy_column]],
        id_vars=[group_col],
        var_name='Type',
        value_name='Traffic Density'
    )

    plt.figure(figsize=(14, 8))
    sns.boxplot(data=melted_df, x='Type', y='Traffic Density', palette='Set2')
    plt.title('Fairness Analysis: Original vs Noisy Traffic Density Distribution', fontsize=16)
    plt.xlabel('Data Type', fontsize=12)
    plt.ylabel('Traffic Density', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

# Region-Wise Fairness Analysis Over Time
def plot_fairness_over_time(df, group_col, noisy_column, original_column, timestamps):
    """
    Plot region-wise traffic density fairness over time.
    """
    fairness_over_time = df[df['timestamp'].isin(timestamps)].groupby([group_col, 'timestamp']).mean()

    plt.figure(figsize=(16, 8))
    sns.lineplot(
        data=fairness_over_time.reset_index(),
        x='timestamp',
        y=noisy_column,
        hue=group_col,
        palette='tab20',
        marker='o'
    )
    plt.title('Region-Wise Fairness Analysis Over Time (Noisy Data)', fontsize=16)
    plt.xlabel('Timestamp (Hour)', fontsize=12)
    plt.ylabel('Noisy Traffic Density', fontsize=12)
    plt.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

# Call the plotting functions
plot_fairness_difference(fairness_analysis)
plot_fairness_boxplot(optimal_noisy_data, original_column='density', noisy_column='noisy_density')
plot_fairness_over_time(optimal_noisy_data, 'region', 'noisy_density', 'density', timestamps=[7, 17])

"""With weather Data

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Define Norwegian cities
norwegian_cities = [
    "Oslo", "Bergen", "Trondheim", "Stavanger", "Kristiansand", "Tromsø", "Sandnes", "Drammen",
    "Fredrikstad", "Porsgrunn", "Ålesund", "Haugesund", "Tønsberg", "Moss", "Bodø", "Arendal",
    "Hamar", "Halden", "Narvik", "Gjøvik", "Lillehammer", "Harstad", "Molde", "Kongsberg",
    "Mo i Rana", "Steinkjer", "Alta", "Sandefjord", "Sarpsborg", "Svolvær", "Vadsø", "Vardø",
    "Røros", "Florø", "Førde", "Voss", "Odda", "Levanger", "Flekkefjord", "Grimstad",
    "Notodden", "Namsos", "Hammerfest", "Kautokeino", "Egersund", "Mandals", "Holmestrand",
    "Stjørdal", "Kongsvinger", "Rakkestad"
]

def simulate_traffic_data_peak_hours(regions, time_intervals, avg_density=100, density_scale=20):
    weather_conditions = ['clear', 'rainy', 'snowy', 'foggy']
    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    data = []
    for region in regions:
        for t in time_intervals:
            weather = np.random.choice(weather_conditions, p=[0.5, 0.2, 0.1, 0.2])
            day = np.random.choice(days_of_week)
            holiday = np.random.choice([True, False], p=[0.05, 0.95])
            temperature = np.random.uniform(-5, 25)  # Simulated temperature range

            if day in days_of_week[:5]:
                if (5.5 <= t < 8 or 16 <= t < 18):  # Typical rush hours
                    density_base = avg_density + 100
                else:
                    density_base = avg_density
            elif day == 'Saturday':
                density_base = avg_density + 50
            else:  # Sunday typically has less traffic
                density_base = avg_density - 50

            if weather == 'rainy' or weather == 'foggy':
                density_base *= 1.1
                if np.random.rand() < 0.05:
                    density_base += 100  # Random chance for accidents
            elif weather == 'snowy':
                density_base *= 1.2
                if np.random.rand() < 0.1:
                    density_base += 100
            elif weather == 'clear':
                density_base += 20

            density_base = max(density_base, 0)  # Ensure non-negative lambda for Poisson distribution
            density = np.random.poisson(lam=density_base + np.random.normal(0, density_scale))
            data.append({
                'region': region, 'timestamp': t, 'density': density, 'weather': weather,
                'temperature': temperature, 'day_of_week': day, 'holiday': holiday
            })
    return pd.DataFrame(data)

# Simulation and saving data
time_intervals = np.arange(0, 24, 1)  # Hourly intervals from 0 to 23
traffic_data = simulate_traffic_data_peak_hours(norwegian_cities, time_intervals)
traffic_data.to_csv('traffic_data_with_factors.csv', index=False)

# Step 3: Query Function
def query_data(df, region, timestamp):
    return df[(df['region'] == region) & (df['timestamp'] == timestamp)]

# Step 4: Global and Local Shuffling
def global_shuffling(df):
    return df.sample(frac=1).reset_index(drop=True)

def local_shuffling(df, group_col):
    shuffled_groups = []
    for _, group_data in df.groupby(group_col):
        shuffled_groups.append(group_data.sample(frac=1).reset_index(drop=True))
    return pd.concat(shuffled_groups)

# Step 5: Inject Laplace Noise for Differential Privacy
def inject_laplace_noise(df, column, epsilon, sensitivity=1.0):
    scale = sensitivity / epsilon
    noise = np.random.laplace(0, scale, len(df))
    df[f'noisy_{column}'] = df[column] + noise
    return df

# Step 6: Experiment with Privacy Budgets
def test_privacy_budgets(df, column, budgets):
    results = []
    for epsilon in budgets:
        noisy_df = inject_laplace_noise(df.copy(), column, epsilon)
        mse = mean_squared_error(df[column], noisy_df[f'noisy_{column}'])
        mae = mean_absolute_error(df[column], noisy_df[f'noisy_{column}'])
        results.append({'epsilon': epsilon, 'MSE': mse, 'MAE': mae})
    return pd.DataFrame(results)

privacy_budgets = np.linspace(0.1, 2.0, 20)
privacy_results = test_privacy_budgets(traffic_data, 'density', privacy_budgets)

# Step 7: Identify Optimal Epsilon
def find_optimal_epsilon(results):
    return results.loc[results['MSE'].idxmin(), 'epsilon']

optimal_epsilon = find_optimal_epsilon(privacy_results)

# Apply optimal epsilon and generate noisy data
optimal_noisy_data = inject_laplace_noise(traffic_data.copy(), 'density', optimal_epsilon)

# Save noisy data to a CSV file (optional)
optimal_noisy_data.to_csv('optimal_noisy_traffic_data.csv', index=False)

# Plot the privacy-utility trade-off with the optimal epsilon highlighted
plot_privacy_tradeoff(privacy_results)

# Fairness analysis function
def analyze_group_representation(df, group_col, original_column, noisy_column):
    original_means = df.groupby(group_col)[original_column].mean()
    noisy_means = df.groupby(group_col)[noisy_column].mean()
    fairness_df = pd.DataFrame({'Original Mean': original_means, 'Noisy Mean': noisy_means})
    fairness_df['Difference'] = fairness_df['Original Mean'] - fairness_df['Noisy Mean']
    return fairness_df

# Perform fairness analysis
fairness_analysis = analyze_group_representation(optimal_noisy_data, 'region', 'density', 'noisy_density')

# Plot fairness comparison
def plot_fairness_comparison(fairness_df):
    fairness_df[['Original Mean', 'Noisy Mean']].plot(kind='bar', figsize=(12, 6))
    plt.title('Fairness Comparison: Original vs Noisy Means')
    plt.xlabel('Region')
    plt.ylabel('Traffic Density')
    plt.xticks(rotation=90)
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_fairness_comparison(fairness_analysis)



# Step 8: Plot Privacy-Utility Trade-Off
def plot_privacy_tradeoff(results):
    plt.figure(figsize=(10, 6))
    plt.plot(results['epsilon'], results['MSE'], label='Mean Squared Error (MSE)', marker='o')
    plt.plot(results['epsilon'], results['MAE'], label='Mean Absolute Error (MAE)', marker='x')
    plt.axvline(x=optimal_epsilon, color='r', linestyle='--', label='Optimal Epsilon')
    plt.title('Privacy-Utility Trade-off')
    plt.xlabel('Privacy Budget (Epsilon)')
    plt.ylabel('Error')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_privacy_tradeoff(privacy_results)

# Step 11: Predict Region-Wise Traffic for Next 24 Hours
def predict_region_traffic(df, regions, timestamps, noisy_column=None):
    predictions = []
    for region in regions:
        for t in timestamps:
            if noisy_column:
                density = df[(df['region'] == region) & (df['timestamp'] == t)][noisy_column].values[0]
            else:
                density = df[(df['region'] == region) & (df['timestamp'] == t)]['density'].values[0]
            predictions.append({'region': region, 'timestamp': t, 'predicted_density': density})
    return pd.DataFrame(predictions)

selected_regions = np.random.choice(regions, size=5, replace=False)
timestamps = np.arange(0, 24)  # Next 24 hours
original_predictions = predict_region_traffic(traffic_data, selected_regions, timestamps)
noisy_predictions = predict_region_traffic(optimal_noisy_data, selected_regions, timestamps, noisy_column='noisy_density')

# Step 12: Combine and Plot Region Predictions
def prepare_combined_predictions(original_df, noisy_df):
    original_df['type'] = 'Original'
    noisy_df['type'] = 'Noisy'
    return pd.concat([original_df, noisy_df])

combined_predictions = prepare_combined_predictions(original_predictions, noisy_predictions)

def plot_region_predictions_combined(combined_df):
    plt.figure(figsize=(14, 8))
    sns.lineplot(data=combined_df, x='timestamp', y='predicted_density', hue='region', style='type', marker='o')
    plt.title('Region-Wise Traffic Predictions for Next 24 Hours')
    plt.xlabel('Timestamp (hour)')
    plt.ylabel('Traffic Density')
    plt.legend(title='Region and Type', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid()
    plt.show()

plot_region_predictions_combined(combined_predictions)



# Additional steps for visualization and analysis such as comparing original and noisy densities, fairness analysis, and traffic predictions can follow the same structure used previously, adapted to the new traffic data with factors.





"""heatmap with weather"""

import numpy as np
import pandas as pd
import folium
from folium.plugins import HeatMapWithTime, HeatMap

# Define cities and their coordinates in Norway
# Define coordinates for Norwegian cities
coordinates = {
    "Oslo": (59.9139, 10.7522),
    "Bergen": (60.3928, 5.3239),
    "Trondheim": (63.4305, 10.3951),
    "Stavanger": (58.9690, 5.7331),
    "Kristiansand": (58.1467, 7.9956),
    "Tromsø": (69.6496, 18.9560),
    "Sandnes": (58.8524, 5.7352),
    "Drammen": (59.7439, 10.2045),
    "Fredrikstad": (59.2181, 10.9298),
    "Porsgrunn": (59.1390, 9.6561),
    "Ålesund": (62.4722, 6.1549),
    "Haugesund": (59.4138, 5.2680),
    "Tønsberg": (59.2675, 10.4076),
    "Moss": (59.4340, 10.6577),
    "Bodø": (67.2804, 14.4049),
    "Arendal": (58.4615, 8.7720),
    "Hamar": (60.7945, 11.0670),
    "Halden": (59.1200, 11.3875),
    "Narvik": (68.4385, 17.4272),
    "Gjøvik": (60.7959, 10.6917)
    # Add more cities as needed
}



# Simulate weather forecast
def simulate_weather_forecast(regions, time_intervals):
    weather_conditions = ['clear', 'rainy', 'snowy', 'foggy']
    forecast = {}
    for region in regions:
        forecast[region] = np.random.choice(weather_conditions, size=len(time_intervals), p=[0.5, 0.2, 0.2, 0.1])
    return forecast

# Simulate traffic data based on weather forecast
def simulate_traffic_data_based_on_weather(regions, time_intervals, weather_forecast, avg_density=100, density_scale=20):
    data = []
    for region in regions:
        for t_idx, t in enumerate(time_intervals):
            weather = weather_forecast[region][t_idx]
            accident = np.random.rand() < 0.05 if weather in ['rainy', 'snowy', 'foggy'] else False
            density_multiplier = {'clear': 1.0, 'rainy': 1.2, 'snowy': 1.5, 'foggy': 1.3}
            density = np.random.poisson(lam=avg_density * density_multiplier[weather] + np.random.normal(0, density_scale))
            data.append({
                'region': region,
                'timestamp': t,
                'density': density,
                'weather': weather,
                'accident': accident
            })
    return pd.DataFrame(data)

# Generate weather forecast and traffic data
time_intervals = np.arange(0, 24)  # Next 24 hours
weather_forecast = simulate_weather_forecast(list(coordinates.keys()), time_intervals)
traffic_data = simulate_traffic_data_based_on_weather(list(coordinates.keys()), time_intervals, weather_forecast)

# Add latitude and longitude for each entry
traffic_data['latitude'] = traffic_data['region'].apply(lambda x: coordinates[x][0])
traffic_data['longitude'] = traffic_data['region'].apply(lambda x: coordinates[x][1])

# Normalize traffic density for better visualization
max_density = traffic_data['density'].max()
traffic_data['normalized_density'] = traffic_data['density'] / max_density

# Heatmap data preparation
heatmap_data = []
for hour in range(24):
    hour_data = traffic_data[traffic_data['timestamp'] == hour]
    heatmap_data.append(hour_data[['latitude', 'longitude', 'normalized_density']].values.tolist())

# Initialize the Folium map
m = folium.Map(location=[65, 15], zoom_start=5)



# Add dynamic heatmap
heatmap = HeatMapWithTime(
    heatmap_data,
    index=[f"{hour}:00" for hour in range(24)],
    auto_play=True,
    max_opacity=0.8
)
heatmap.add_to(m)

# Add markers for recent accidents within the last 6 hours
recent_accidents = traffic_data[(traffic_data['accident'] == True) & (traffic_data['timestamp'] >= 18)]
for _, row in recent_accidents.iterrows():
    folium.CircleMarker(
        location=[row['latitude'], row['longitude']],
        radius=5,
        popup=f"Accident in {row['weather']} weather at {row['region']}",
        color='red',
        fill=True,
        fill_color='red'
    ).add_to(m)

# Save and display the map
m.save('traffic_heatmap_with_accidents.html')
m

import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch



# Create a new figure for the enhanced diagram with even larger fonts
fig, ax = plt.subplots(figsize=(12, 15))

# Define the three main layers with bigger font sizes and better clarity
layers = [
    {
        "name": "IoT Devices Layer",
        "xy": (0.2, 0.78),
        "color": "#FFC0CB",
        "desc": "1. Collect raw traffic data\n2. Apply local & global iterative shuffling\n3. Define Query Function Q:\n   - Extract Relevant Data\n   - Input: Dataset D, Constraints C\n   - Output: Data Subset R"
    },
    {
        "name": "Edge Computing Layer",
        "xy": (0.2, 0.50),
        "color": "#98FB98",
        "desc": "1. Perform preliminary data processing\n2. Execute queries on aggregated data. \n3. Iterative Shuffling: a)Devide dataset into groups ,\n b)Perform local shuffling within the groups,\n c) perform global shuffling among the groups. \n4. Apply differential privacy:\n   - Input: Sensitivity Δf, Privacy Budget ε\n   - Output: Noisy Dataset R''. \n5. check Fairness: -check propostional \n representation among groups"
    },
    {
        "name": "Cloud Layer",
        "xy": (0.2, 0.15),
        "color": "#ADD8E6",
        "desc": "1. Store anonymized & aggregated data\n2. Perform advanced analysis\n3. Privacy Budget Allocation:\n   - Distribute ε_T across queries Q_1, Q_2, ..., Q_m\n4. Optimization:\n   - Minimize Loss Function L(U, P)\n   - Output: Optimized final report with maximum utility. \n5. Recheck Fairness"
    }
]

# Add layers to the plot with larger fonts
for layer in layers:
    rect = FancyBboxPatch(
        layer["xy"], 0.6, 0.17,
        boxstyle="round,pad=0.1", edgecolor="black", facecolor=layer["color"], linewidth=1.5
    )
    ax.add_patch(rect)
    # Add text for layer name and description with larger font sizes
    cx = layer["xy"][0] + 0.3
    cy = layer["xy"][1] + 0.1
    ax.text(cx, cy + 0.08, layer["name"], ha="center", va="center", fontsize=20, fontweight="bold")
    ax.text(cx, cy - 0.04, layer["desc"], ha="center", va="center", fontsize=16, wrap=True, color="black")

# Add title and remove axes for clean visualization
ax.set_title("Privacy-Utility-Fairness balanced LBTS framework", fontsize=24, fontweight="bold", pad=20)
ax.axis('off')

# Show the updated and more visually appealing plot
plt.show()

# Generate historical traffic data
def generate_historical_data(regions, days=30):
    data = []
    for day in range(days):
        date = datetime.now() - timedelta(days=day)
        weekday = date.weekday()
        for hour in range(24):
            for region in regions:
                peak_hour = 7 <= hour <= 10 or 15 <= hour <= 17.5
                scaling_factor = 1.5 if peak_hour else 0.8
                scaling_factor *= 1.2 if weekday < 5 else 0.9
                base_vehicles = np.random.randint(1000, 8000)
                vehicles = base_vehicles * scaling_factor
                weather = np.random.choice(['clear', 'rainy', 'snowy', 'foggy'], p=[0.6, 0.2, 0.1, 0.1])
                road_condition = np.random.choice(['good', 'moderate', 'poor'], p=[0.6, 0.3, 0.1])
                road_impact = 1.0 if road_condition == 'good' else 1.2 if road_condition == 'moderate' else 1.5
                vehicles *= road_impact
                accident_probability = 0.05
                if weather in ['rainy', 'snowy', 'foggy']:
                    accident_probability += 0.1
                if vehicles > 5000:
                    accident_probability += 0.1
                if weekday < 5:
                    accident_probability += 0.05
                accident = np.random.rand() < accident_probability
                delay = 0
                if accident:
                    delay += np.random.randint(5, 30)
                delay += int(vehicles / 1000)
                data.append([
                    region, date.date(), hour, vehicles, weather, weekday, accident, delay
                ])
    return pd.DataFrame(data, columns=['region', 'date', 'hour', 'vehicles', 'weather', 'weekday', 'accident', 'delay'])

# Generate 30 days of historical data
historical_data = generate_historical_data(list(coordinates.keys()), days=30)
historical_data.head()



# Save the generated data to a CSV file
historical_data.to_csv('historical_traffic_data.csv', index=False)

import matplotlib.pyplot as plt
import base64
from io import BytesIO

# Function to generate and save a traffic prediction graph for each region
def generate_traffic_graph(region, region_hourly_data):
    hours = range(24)
    vehicle_counts = [int(region_hourly_data[region_hourly_data['hour'] == hour]['vehicles'].values[0]) for hour in hours]

    # Create a plot
    plt.figure(figsize=(6, 4))
    plt.plot(hours, vehicle_counts, marker='o', color='blue', label='Predicted Traffic')
    plt.axvspan(7, 10, color='yellow', alpha=0.3, label='Peak Hours (Morning)')
    plt.axvspan(15, 17.5, color='orange', alpha=0.3, label='Peak Hours (Evening)')
    plt.title(f"Traffic Prediction for {region}")
    plt.xlabel("Hour")
    plt.ylabel("Number of Vehicles")
    plt.xticks(hours)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend(loc='upper left')

    # Save the plot as a base64-encoded PNG
    buf = BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight')
    buf.seek(0)
    image_base64 = base64.b64encode(buf.read()).decode('utf-8')
    buf.close()
    plt.close()

    return f'<img src="data:image/png;base64,{image_base64}" width="300" height="200"/>'

# Add markers with traffic prediction graphs for each region
for region, coord in coordinates.items():
    region_hourly_data = historical_data[historical_data['region'] == region]
    traffic_graph_html = generate_traffic_graph(region, region_hourly_data)

    # Add marker with the graph
    folium.Marker(
        location=coord,
        popup=folium.Popup(f"""
            <b>Region:</b> {region}<br>
            <b>Traffic Prediction:</b><br>
            {traffic_graph_html}
        """, max_width=400),
        icon=folium.Icon(color='blue', icon='info-sign')
    ).add_to(m)

# Save the map to an HTML file
m.save('traffic_prediction_with_graphs.html')
m

import numpy as np
import pandas as pd
import folium
from folium.plugins import HeatMapWithTime
import matplotlib.pyplot as plt
from io import BytesIO
import base64
from datetime import datetime, timedelta

# Define cities and their coordinates in Norway
coordinates = {
    "Oslo": (59.9139, 10.7522),
    "Bergen": (60.3928, 5.3239),
    "Trondheim": (63.4305, 10.3951),
    "Stavanger": (58.9690, 5.7331),
    "Kristiansand": (58.1467, 7.9956),
    "Tromsø": (69.6496, 18.9560),
    "Sandnes": (58.8524, 5.7352),
    "Drammen": (59.7439, 10.2045),
    "Fredrikstad": (59.2181, 10.9298),
    "Porsgrunn": (59.1390, 9.6561),
    "Ålesund": (62.4722, 6.1549),
    "Haugesund": (59.4138, 5.2680),
    "Tønsberg": (59.2675, 10.4076),
    "Moss": (59.4340, 10.6577),
    "Bodø": (67.2804, 14.4049),
    "Arendal": (58.4615, 8.7720),
    "Hamar": (60.7945, 11.0670),
    "Halden": (59.1200, 11.3875),
    "Narvik": (68.4385, 17.4272),
    "Gjøvik": (60.7959, 10.6917)
}

# Simulate historical traffic data
def generate_historical_data(regions, days=30):
    data = []
    for day in range(days):
        date = datetime.now() - timedelta(days=day)
        weekday = date.weekday()
        for hour in range(24):
            for region in regions:
                peak_hour = 7 <= hour <= 10 or 15 <= hour <= 17.5
                scaling_factor = 1.5 if peak_hour else 0.8
                scaling_factor *= 1.2 if weekday < 5 else 0.9
                base_vehicles = np.random.randint(1000, 8000)
                vehicles = base_vehicles * scaling_factor
                weather = np.random.choice(['clear', 'rainy', 'snowy', 'foggy'], p=[0.6, 0.2, 0.1, 0.1])
                road_condition = np.random.choice(['good', 'moderate', 'poor'], p=[0.6, 0.3, 0.1])
                road_impact = 1.0 if road_condition == 'good' else 1.2 if road_condition == 'moderate' else 1.5
                vehicles *= road_impact
                accident_probability = 0.05
                if weather in ['rainy', 'snowy', 'foggy']:
                    accident_probability += 0.1
                if vehicles > 5000:
                    accident_probability += 0.1
                if weekday < 5:
                    accident_probability += 0.05
                accident = np.random.rand() < accident_probability
                delay = 0
                if accident:
                    delay += np.random.randint(5, 30)
                delay += int(vehicles / 1000)
                data.append([
                    region, date.date(), hour, vehicles, weather, weekday, accident, delay
                ])
    return pd.DataFrame(data, columns=['region', 'date', 'hour', 'vehicles', 'weather', 'weekday', 'accident', 'delay'])

# Apply differential privacy
def apply_differential_privacy(value, sensitivity=1, epsilon=1):
    scale = sensitivity / epsilon
    noise = np.random.laplace(0, scale)
    return max(0, value + noise)

# Generate historical traffic data
historical_data = generate_historical_data(list(coordinates.keys()), days=30)

# Generate noisy predictions
noisy_predictions = {
    region: [apply_differential_privacy(row['vehicles'], sensitivity=10, epsilon=0.5)
             for _, row in historical_data[historical_data['region'] == region].iterrows()]
    for region in coordinates.keys()
}
def generate_prediction_graph(region, hourly_predictions):
    # Generate graph for predictions over 24 hours
    hours = range(24)
    predictions = hourly_predictions[:24]  # Ensure 24-hour predictions are used
    plt.figure(figsize=(6, 4))
    plt.plot(hours, predictions, marker='o', color='blue', label='Predicted Traffic')
    plt.title(f"Traffic Prediction for {region}")
    plt.xlabel("Hour")
    plt.ylabel("Number of Vehicles")
    plt.grid(True)
    plt.axvspan(7, 10, color='yellow', alpha=0.3, label='Peak Hours (Morning)')
    plt.axvspan(15, 17.5, color='orange', alpha=0.3, label='Peak Hours (Evening)')
    plt.legend()
    buf = BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight')
    buf.seek(0)
    plt.close()
    return base64.b64encode(buf.getvalue()).decode('utf-8')


# Select accident regions for a single hour
selected_hour = 21  # Choose a specific hour for accident occurrence
accident_data = historical_data[(historical_data['hour'] == selected_hour) & (historical_data['accident'])]
selected_accident_regions = accident_data['region'].unique()

# Limit accidents to one or two regions
if len(selected_accident_regions) > 2:
    selected_accident_regions = np.random.choice(selected_accident_regions, size=2, replace=False)

# Initialize map
m = folium.Map(location=[62, 10], zoom_start=6, tiles="cartodbpositron")

# Add markers for regions
for region, coord in coordinates.items():
    # Filter region data from historical data
    region_data = historical_data[historical_data['region'] == region]
    hourly_predictions = noisy_predictions[region][:24]  # Extract 24-hour noisy predictions

    if region in selected_accident_regions:
        # Accident marker logic
        accident_row = region_data[region_data['hour'] == selected_hour].iloc[0]
        accident_popup = f"""
            <b>Region:</b> {region}<br>
            <b>Accident Reported!</b><br>
            <b>Time:</b> {selected_hour}:00<br>
            <b>Weather:</b> {accident_row['weather']}<br>
            <b>Vehicles:</b> {int(accident_row['vehicles'])}<br>
            <b>Delay:</b> {int(accident_row['delay'])} minutes
        """
        folium.Marker(
            location=coord,
            popup=folium.Popup(accident_popup, max_width=300),
            icon=folium.Icon(color='red', icon='exclamation-triangle', prefix='fa')
        ).add_to(m)
    else:
        # Blue marker for prediction graph
        graph_base64 = generate_prediction_graph(region, hourly_predictions)
        traffic_popup = f"""
            <b>Region:</b> {region}<br>
            <b>Traffic Prediction:</b><br>
            <img src="data:image/png;base64,{graph_base64}" width="300">
        """
        folium.Marker(
            location=coord,
            popup=folium.Popup(traffic_popup, max_width=400),
            icon=folium.Icon(color='blue', icon='info-sign')
        ).add_to(m)


# Save the map
m.save('accidents.html')
m

"""#With LSTM

"""

import numpy as np
import pandas as pd
import folium
from folium.plugins import HeatMapWithTime
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta

# Define cities and their coordinates in Norway
coordinates = {
    "Oslo": (59.9139, 10.7522),
    "Bergen": (60.3928, 5.3239),
    "Trondheim": (63.4305, 10.3951),
    "Stavanger": (58.9690, 5.7331),
    "Kristiansand": (58.1467, 7.9956),
    "Tromsø": (69.6496, 18.9560),
    "Sandnes": (58.8524, 5.7352),
    "Drammen": (59.7439, 10.2045),
    "Fredrikstad": (59.2181, 10.9298),
    "Porsgrunn": (59.1390, 9.6561),
    "Ålesund": (62.4722, 6.1549),
    "Haugesund": (59.4138, 5.2680),
    "Tønsberg": (59.2675, 10.4076),
    "Moss": (59.4340, 10.6577),
    "Bodø": (67.2804, 14.4049),
    "Arendal": (58.4615, 8.7720),
    "Hamar": (60.7945, 11.0670),
    "Halden": (59.1200, 11.3875),
    "Narvik": (68.4385, 17.4272),
    "Gjøvik": (60.7959, 10.6917)
}

# Simulate historical traffic data
def generate_historical_data(regions, days=30):
    data = []
    for day in range(days):
        date = datetime.now() - timedelta(days=day)
        weekday = date.weekday()
        for hour in range(24):
            for region in regions:
                peak_hour = 7 <= hour <= 10 or 15 <= hour <= 17.5
                scaling_factor = 1.5 if peak_hour else 0.8
                scaling_factor *= 1.2 if weekday < 5 else 0.9
                base_vehicles = np.random.randint(1000, 8000)
                vehicles = base_vehicles * scaling_factor
                weather = np.random.choice(['clear', 'rainy', 'snowy', 'foggy'], p=[0.6, 0.2, 0.1, 0.1])
                road_condition = np.random.choice(['good', 'moderate', 'poor'], p=[0.6, 0.3, 0.1])
                road_impact = 1.0 if road_condition == 'good' else 1.2 if road_condition == 'moderate' else 1.5
                vehicles *= road_impact
                accident_probability = 0.05
                if weather in ['rainy', 'snowy', 'foggy']:
                    accident_probability += 0.1
                if vehicles > 5000:
                    accident_probability += 0.1
                if weekday < 5:
                    accident_probability += 0.05
                accident = np.random.rand() < accident_probability
                delay = 0
                if accident:
                    delay += np.random.randint(5, 30)
                delay += int(vehicles / 1000)
                data.append([
                    region, date.date(), hour, vehicles, weather, weekday, accident, delay
                ])
    return pd.DataFrame(data, columns=['region', 'date', 'hour', 'vehicles', 'weather', 'weekday', 'accident', 'delay'])

# Prepare historical data
historical_data = generate_historical_data(list(coordinates.keys()), days=60)

# Prepare data for LSTM
def prepare_lstm_data(historical_data, regions):
    lstm_data = []
    for region in regions:
        region_data = historical_data[historical_data['region'] == region]
        region_data = region_data.sort_values(by=['date', 'hour'])
        lstm_data.append(region_data['vehicles'].values)
    return np.array(lstm_data)

# Normalize data
scaler = MinMaxScaler()
lstm_data = prepare_lstm_data(historical_data, list(coordinates.keys()))
scaled_data = np.array([scaler.fit_transform(region_data.reshape(-1, 1)) for region_data in lstm_data])

# Prepare LSTM model
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))
    model.add(LSTM(50, return_sequences=False))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Train LSTM model
X_train, y_train = [], []
time_steps = 24
for region_data in scaled_data:
    for i in range(len(region_data) - time_steps):
        X_train.append(region_data[i:i+time_steps])
        y_train.append(region_data[i+time_steps])
X_train, y_train = np.array(X_train), np.array(y_train)
lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]))
lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

# Predict traffic for the next 24 hours
def predict_traffic(regions, scaler, lstm_model):
    predictions = {}
    for region in regions:
        region_data = scaled_data[regions.index(region)]
        last_24_hours = region_data[-24:]
        prediction_input = last_24_hours.reshape(1, -1, 1)
        predicted_traffic = lstm_model.predict(prediction_input)
        predictions[region] = [scaler.inverse_transform([[val]])[0][0] for val in predicted_traffic.flatten()]
    return predictions

# Generate predicted data
predicted_traffic = predict_traffic(list(coordinates.keys()), scaler, lstm_model)

# Validate predictions to ensure 24-hour data
for region, predictions in predicted_traffic.items():
    if len(predictions) != 24:
        predicted_traffic[region] = (predictions + [0] * 24)[:24]

# Add differential privacy
def apply_differential_privacy(value, sensitivity=1, epsilon=1):
    scale = sensitivity / epsilon
    noise = np.random.laplace(0, scale)
    return max(0, value + noise)

noisy_predictions = {
    region: [apply_differential_privacy(value, sensitivity=10, epsilon=0.5) for value in traffic]
    for region, traffic in predicted_traffic.items()
}

# Ensure noisy predictions have 24-hour values
for region, predictions in noisy_predictions.items():
    if len(predictions) != 24:
        noisy_predictions[region] = (predictions + [0] * 24)[:24]

# Create heatmap data
heatmap_data = []
for hour in range(24):
    hour_data = []
    for region, coord in coordinates.items():
        traffic = noisy_predictions[region][hour]
        hour_data.append([coord[0], coord[1], traffic])
    heatmap_data.append(hour_data)

# Initialize the map
m = folium.Map(location=[62, 10], zoom_start=6, tiles="cartodbpositron")

# Normalize the traffic data for better visualization
max_traffic = max([max(values) for values in noisy_predictions.values()])
heatmap_data = []

for hour in range(24):
    hour_data = []
    for region, coord in coordinates.items():
        try:
            traffic = noisy_predictions[region][hour]
        except IndexError:
            traffic = 0  # Default value for missing data
        normalized_traffic = traffic / max_traffic  # Normalize traffic
        hour_data.append([coord[0], coord[1], normalized_traffic])
    heatmap_data.append(hour_data)

# Add dynamic heatmap to the map
heatmap = HeatMapWithTime(
    heatmap_data,
    index=[f"{hour}:00" for hour in range(24)],
    auto_play=True,
    max_opacity=0.8,
    gradient={0: 'green', 0.5: 'orange', 1: 'red'}
)
heatmap.add_to(m)

# Add accident markers with better visualization
for _, row in historical_data[historical_data['accident']].iterrows():
    folium.Marker(
        location=coordinates[row['region']],
        popup=f"<b>Accident Reported</b><br>Region: {row['region']}<br>Weather: {row['weather']}<br>Vehicles: {int(row['vehicles'])}<br>Delay: {row['delay']} mins",
        icon=folium.Icon(color='red', icon='exclamation-triangle', prefix='fa')
    ).add_to(m)


# Add regional markers with details
for region, coord in coordinates.items():
    avg_traffic = int(np.mean(noisy_predictions[region]))  # Calculate average traffic
    folium.Marker(
        location=coord,
        popup=f"<b>Region:</b> {region}<br><b>Average Traffic:</b> {avg_traffic} vehicles",
        icon=folium.Icon(color='blue', icon='info-sign')
    ).add_to(m)

# Add custom CSS to style the heatmap and popups
custom_css = """
<style>
    .leaflet-popup-content-wrapper {
        background-color: rgba(255, 255, 255, 0.9);
        border-radius: 10px;
        box-shadow: 0 0 15px rgba(0, 0, 0, 0.2);
    }
    .leaflet-popup-content {
        font-size: 14px;
        font-weight: bold;
        color: #333;
    }
</style>
"""
m.get_root().html.add_child(folium.Element(custom_css))

# Save the map to an HTML file
m.save('enhanced_traffic_map.html')
m